{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LaCE DOCUMENTATION","text":"<p>Welcome to the documentation for LaCE! LaCE contains a set of emulators for the one-dimensional flux power spectrum of the Lyman-alpha forest. It has been used in the papers:</p> <ul> <li>https://arxiv.org/abs/2011.15127</li> <li>https://arxiv.org/abs/2209.09895</li> <li>https://arxiv.org/abs/2305.19064 (latest version)</li> </ul> <p>Please cite at least https://arxiv.org/abs/2305.19064 if you use this emulator in your research.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure that the following software is installed on your system:</p> <ul> <li>Python 3.10</li> <li>pip</li> <li>Git</li> </ul> <p>You will also need to clone the repository to your local environment by executing the following commands:</p> <pre><code>git clone https://github.com/igmhub/LaCE\n</code></pre>"},{"location":"installation/","title":"INSTALLATION","text":"<p>(Last updated: Nov 19 2024)</p> <p>LaCE contains a submodule to estimate compressed parameters from the power spectrum that uses cosmopower. The LaCE installation is slightly different depending on whether you want to use cosmopower or not.</p>"},{"location":"installation/#lace-without-cosmopower","title":"LaCE without cosmopower","text":"<ul> <li>Create a new conda environment. It is usually better to follow python version one or two behind. In January 2024, the latest is 3.12, so we recommend 3.11. If you want to use LaCE with cosmopower, as of November 2025 you need to install python 3.10. Please look at the cosmopower installation before proceeding with the LaCE installation.</li> </ul> <pre><code>conda create -n lace -c conda-forge python=3.11 pip \nconda activate lace\npip install --upgrade pip\n</code></pre> <ul> <li>Clone the repo into your machine and perform an editable installation:</li> </ul> <pre><code>git clone https://github.com/igmhub/LaCE.git\ncd LacE\npip install -e .\n</code></pre> <ul> <li>If you find problems, please install LaCE as follows:</li> </ul> <pre><code>pip install -e \".[explicit]\"\n</code></pre>"},{"location":"installation/#lace-with-cosmopower","title":"LaCE with cosmopower","text":"<ul> <li>Create a new conda environment. </li> </ul> <pre><code>conda create -n lace -c conda-forge python=3.11 pip \nconda activate lace\npip install --upgrade pip\n</code></pre> <ul> <li>Install cosmopower:</li> </ul> <pre><code>pip install cosmopower pyDOE\n</code></pre> <ul> <li>Clone the repo into your machine and perform an editable installation:</li> </ul> <pre><code>git clone https://github.com/igmhub/LaCE.git\ncd LacE\n</code></pre> <ul> <li>Install LaCE using the installation with explicit dependencies:</li> </ul> <pre><code>pip install -e \".[explicit]\"\n</code></pre>"},{"location":"developers/","title":"FOR DEVELOPERS","text":"<p>Welcome to the LaCE developer documentation! This section contains information for developers who want to contribute to LaCE or understand its internals better.</p>"},{"location":"developers/#contents","title":"Contents","text":"<ul> <li>Creating New Emulators: Learn how to create and add new emulator types to LaCE</li> <li>Training Options: Implemented solutions to improve the emulators performance</li> <li>Code Testing: Information to mantain and extend the automated testing</li> <li>Documentation: How to write and maintain documentation</li> </ul>"},{"location":"developers/#getting-started","title":"Getting Started","text":"<p>If you're new to developing for LaCE, we recommend:</p> <ol> <li>Reading the installation instructions</li> <li>Setting up your development environment</li> <li>...</li> </ol> <p>For any questions, please open an issue on GitHub or reach out to the maintainers.</p>"},{"location":"developers/CreateNewEmulator/","title":"CREATING NEW EMULATORS","text":"<p>The training of the emulators is done with the code <code>train.py</code>. This code is used to train custom emulators already defined with an emulator label. However, you might need to define a new emulator label with new hyperparameters. This tutorial will guide you through the process of creating a new emulator label.</p> <p>The file <code>lace/emulator/constants.py</code> contains the definitions of the emulator labels, training sets, and the emulator parameters associated with each emulator label.</p> <p>To create a new emulator label, you first need to add your new emulator label to the <code>EmulatorLabel</code> class in the <code>constants.py</code> file, for example:</p> <pre><code>class EmulatorLabel(StrEnum):\n    ...\n    NEW_EMULATOR = \"New_Emulator\"\n</code></pre> <p>\"New emulator\" is the name of the new emulator label that identifies it in the emulator calls, e.g. <code>NNEmulator(emulator_label=\"New_Emulator\")</code>.</p> <p>Then this label needs to be added to <code>GADGET_LABELS</code> or <code>NYX_LABELS</code> in the <code>constants.py</code> file, depending on the training set you used to train your emulator. For example, if this is a new Gadget emulator, you need to add it to <code>GADGET_LABELS</code>:</p> <pre><code>GADGET_LABELS = {\n    ...\n    EmulatorLabel.NEW_EMULATOR,\n}\n</code></pre> <p>The dictionary <code>EMULATOR_PARAMS</code> also needs to be updated with the new emulator parameters. Here, one needs to add all the arguments needed to initialize the emulator class. For example:</p> <pre><code>    \"Nyx_alphap_cov\": {\n        \"emu_params\": [\n            \"Delta2_p\",\n            \"n_p\",\n            \"alpha_p\",\n            \"mF\",\n            \"sigT_Mpc\",\n            \"gamma\",\n            \"kF_Mpc\",\n        ],\n        \"emu_type\": \"polyfit\",\n        \"kmax_Mpc\": 4,\n        \"ndeg\": 6,\n        \"nepochs\": 600,\n        \"step_size\": 500,\n        \"nhidden\": 6,\n        \"max_neurons\": 400,\n        \"lr0\": 2.5e-4,\n        \"weight_decay\": 8e-3,\n        \"batch_size\": 100,\n        \"amsgrad\": True,\n        \"z_max\": 5,\n        \"include_central\": False,\n    }\n</code></pre> <p>Finally, you need to add a description of the new emulator in the <code>EMULATOR_DESCRIPTIONS</code> dictionary:</p> <pre><code>EMULATOR_DESCRIPTIONS = {\n    ...\n    EmulatorLabel.NEW_EMULATOR: \"Description of the new emulator\",\n}\n</code></pre> <p>With this, you have added a new emulator label to the code! You should be able to train your new emulator with the command:</p> <pre><code>python scripts/train.py --config=path/to/config.yaml\n</code></pre> <p>or call the emulator directly with:</p> <pre><code>emulator = NNEmulator(emulator_label=\"New_Emulator\",\n                      archive=archive)\n</code></pre>"},{"location":"developers/CreateNewEmulator/#loading-the-new-emulator","title":"Loading the new emulator","text":"<p>Once you have defined a new emulator label, you might want to save the trained emulator models and load them without the need of retraining. This can be done either specifying the <code>model_path</code> argument when initializing the emulator. </p> <pre><code>emulator = NNEmulator(emulator_label=\"New_Emulator\",\n                      model_path=\"path/to/model.pt\",\n                      train=False,\n                      archive=archive)\n</code></pre> <p>And also using the <code>emulator_manager</code> function:</p> <pre><code>emulator = emulator_manager(emulator_label=\"New_Emulator\"\n                            archive=archive)\n</code></pre> <p>In the first case, since you are specifying the <code>model path</code>, there is no naming convention for the model file. However, in the second case, the saved models must be stored in the following way:</p> <ul> <li>The folder must be  <code>data/NNmodels/</code> from the root of the repository.</li> <li>For a specific emulator label, you need to create a new folder, e.g. <code>New_Emulator</code>.</li> <li>For the emulator using all training simulations, the model file is named <code>New_Emulator.pt</code>.</li> <li>For the emulator using the training set excluding a given simulation, the model file is named <code>New_Emulator_drop_sim_{simulation suite}_{simulation index}.pt</code>. For example, if you exclude the 10th simulation from the mpg training set, the model file is named <code>New_Emulator_drop_sim_mpg_10.pt</code>.   </li> </ul> <p>For this reason, if the emulator label is provided, it will be saved following the naming convention even if another model path is specified.</p> <p>The emulator manager will automatically find the correct model file for the given emulator label. To set this up, you need to add the new emulator label to the <code>folder</code> dictionary in the <code>emulator_manager.py</code> file.</p> <pre><code>folder = {\n    ...\n    EmulatorLabel.NEW_EMULATOR: \"NNmodels/New_Emulator/\",\n}\n</code></pre>"},{"location":"developers/UnderDevelopment/","title":"UNDER DEVELOPMENT SOLUTIONS","text":"<p>There are several features that can be used to customize the training of the emulators. This tutorial will guide you through the process of training emulators with different options.</p> <ul> <li>Weighting with a covariance matrix</li> <li>Weighting simulations depending of the scalings (mean flux, temperature )</li> </ul>"},{"location":"developers/UnderDevelopment/#weighting-with-a-covariance-matrix","title":"Weighting with a covariance matrix","text":"<p>The emulator supports weighting the training simulations with a covariance matrix. This covariance matrix is used to weight the training simulations during the training of the neural network.</p> <p>To train an emulator with a covariance matrix, you need to provide a covariance matrix for the training simulations. Currently, the emulator only supports a diagonal covariance matrix. It is important that the covariance matrix is given in the k binning of the training simulations.</p> <p>The function <code>_load_DESIY1_err</code> in the <code>nn_emulator.py</code> file loads a covariance matrix. The covariance must be a json file with the relative error as a function of z for each k bin.</p> <p>From the relative error file in <code>data/DESI_cov/rel_err_DESI_Y1.npy</code>, we can generate the json file with the following steps:</p> <p>First we load the data from the relative error file:</p> <pre><code>cov =  np.load(PROJ_ROOT / \"data/DESI_cov/rel_error_DESIY1.npy\", allow_pickle=True)\n# Load the data dictionary\ndata = cov.item()\n</code></pre> <p>Then we extract the arrays. This has a hidden important step. In the original relative error file, the values for the relative error are set to 100 correspond to the scales not measured by DESI. The value of 100 is set at random, and can be optimized for the training. Initial investigations indicated that setting the value to 5 was working well. However, this parameter could be furtehr refined. Currently is set to 5, but other values of this dummy value could be used.</p> <pre><code># Extract the arrays\nz_values = data['z']\nrel_error_Mpc = data['rel_error_Mpc']\nrel_error_Mpc[rel_error_Mpc == 100] = 5\n\nk_cov = data['k_Mpc']\n</code></pre> <p>Then we extract the k values for the training simulations to ensure that the covariance matrix is given in the k binning of the training simulations.</p> <pre><code>testing_data_central = archive.get_testing_data('nyx_central')\ntesting_data = archive.get_testing_data('nyx_0')\nk_Mpc_LH = testing_data[0]['k_Mpc'][testing_data[0]['k_Mpc']&lt;4]\n</code></pre> <p>And then we create the dictionary with the relative error as a function of z for each k bin:</p> <pre><code># Load the data dictionary\ndata = cov.item()\nz_values = data['z']\n\ndict_={}\nfor z, rel_error_row in zip(z_values, rel_error_Mpc):\n    f = interp1d(k_cov, rel_error_row, fill_value=\"extrapolate\")\n    rel_error_Mpc_interp = f(k_Mpc_LH)\n    rel_error_Mpc_interp[0:3] = rel_error_Mpc_interp[3]\n    dict_[f\"{z}\"]=rel_error_Mpc_interp.tolist()\n\n# Create a new dictionary with z as keys and corresponding rel_error_Mpc rows as values\n#z_to_rel_error_serializable = {float(z): rel_error_row.tolist() for z, rel_error_row in z_to_rel_error.items()}\n</code></pre> <p>And finally we save the dictionary to a json file:</p> <pre><code># Save the z_to_rel_error dictionary to a JSON file\nwith open(PROJ_ROOT / \"data/DESI_cov/rerr_DESI_Y1.json\", \"w\") as json_file:\n    json.dump(dict_, json_file, indent=4)\n</code></pre>"},{"location":"developers/UnderDevelopment/#weighting-simulations-depending-of-the-scalings-mean-flux-temperature","title":"Weighting simulations depending of the scalings (mean flux, temperature )","text":"<p>The <code>nn_emulator.py</code> file contains a function <code>_get_rescalings_weights</code> that allows to weight the simulations depending on the scalings. This can be used to give more importance to the snapshots with certain scalings. It is possible to weight differently based on the scaling value and the redshift. Initial investigations did not show an improvement in the emulator performance when weighting the simulations. However, might be worth to further investigate this option.</p> <p>The function <code>_get_rescalings_weights</code> can be customized by changing the line:</p> <pre><code>weights_rescalings[np.where([(d['val_scaling'] not in [0,1] and d['z'] in [2.8, 3,3.2,3.4]) for d in self.training_data])] = 1\n</code></pre> <p>The weight value of 1 does not have any effect on the training. To downweight certain snapshots, a value lower than 1 can be used. In this particular case, modifying it to a lower value, for example 0.5, would downweight the snapshots with a scaling value not equal to 0 or 1 (temparature scalings) and a redshift in the range [2.8, 3,3.2,3.4].</p> <p>Initial investigations showed that very low values of the weights, for example 0.01 already led to a similar performance to the one of an emulator trained with equal weights.</p>"},{"location":"developers/advancedTesting/","title":"MAINTAINING THE AUTOMATED TESTING","text":"<p>LaCE uses automated testing to ensure code quality and prevent regressions. This guide explains how to maintain and extend the test suite. This section is intended for developers who are maintaining the automated testing.</p>"},{"location":"developers/advancedTesting/#running-tests","title":"Running Tests","text":"<p>Automated tests are run using pytest. The tests pipeline is at <code>.github/workflows/python-tests.yml</code>. To add another test, you have to:</p> <ol> <li>In the section <code>Run tests</code>, in <code>.github/workflows/python-tests.yml</code>, add the command to run your test.</li> </ol> <pre><code>    - name: Run tests\n      run: |\n        ...\n        pytest tests/test_your_test.py\n        pytest tests/test_your_other_test.py\n</code></pre> <ol> <li>Add the script with your test in the <code>tests</code> folder.</li> <li>The testing function must start with <code>test_</code> (e.g., <code>test_my_function</code>). Tests can take fixtures as arguments.</li> </ol> <p>In the <code>.github/workflows/python-tests.yml</code> file, you can specify when the test should be run. For example, currently tests are only run after a PR to the <code>main</code> branch.</p> <pre><code>    on:\n    push:\n        branches: 'main'\n</code></pre> <p>When a PR is merged into the <code>main</code> branch, the tests are run automatically at Github Actions.</p>"},{"location":"developers/documentation/","title":"MAINTAINING THE DOCUMENTATION","text":"<p>LaCE uses <code>mkdocs</code> to build the documentation. The documentation is hosted at LaCE documentation. The documentation can be built locally using the following command:</p> <pre><code>mkdocs build\n</code></pre> <p>and then served using</p> <pre><code>mkdocs serve\n</code></pre> <p>The documentation is pushed to the <code>gh-pages</code> branch at each release (merge into <code>main</code>). The <code>gh-pages</code> branch is automatically updated when a PR is merged into <code>main</code>.</p> <p>In order to write documentation, you can use the following structure:</p> <ul> <li><code>docs/docs/developers</code>: Documentation for developers</li> <li><code>docs/docs/users</code>: Documentation for users</li> </ul> <p>To add a new page, you should create a new <code>.md</code> file to the <code>docs/docs/</code> folder.  To define where this document should be included and the structure of the documentation, add the new page to the <code>mkdocs.yml</code>. The new page will automatically be added to the navigation menu.  To have a cleaner structure, add the new page to the corresponding <code>index.md</code> file. The documentation is structured with an index file for each section.</p>"},{"location":"users/","title":"USER GUIDE","text":"<p>Welcome to the LaCE user documentation! This section contains information for users who want to use LaCE to make predictions or train new emulators.</p>"},{"location":"users/#contents","title":"Contents","text":"<ul> <li>Available simulations</li> <li>Predefined emulators and training sets</li> <li>Loading data from archive</li> <li>Making predictions with emulators</li> <li>Training new emulators</li> <li>Compressed Parameters</li> </ul>"},{"location":"users/Emulators_trainingSets/","title":"PREDEFINED EMULATORS AND TRAINING SETS","text":""},{"location":"users/Emulators_trainingSets/#predefined-emulators","title":"PREDEFINED EMULATORS","text":"<p>LaCE provides a set of predefined emulators that have been validated. These emulators are:</p> <ul> <li> <p>Neural network emulators:</p> <ul> <li>Gadget emulators: <ul> <li>Cabayol23: Neural network emulating the optimal P1D of Gadget simulations fitting coefficients to a 5th degree polynomial. It goes to scales of 4Mpc^{-1} and z&lt;=4.5.</li> <li>Cabayol23+: Neural network emulating the optimal P1D of Gadget simulations fitting coefficients to a 5th degree polynomial. It goes to scales of 4Mpc^{-1} and z&lt;=4.5. Updated version compared to Cabayol+23 paper.</li> <li>Cabayol23_extended: Neural network emulating the optimal P1D of Gadget simulations fitting coefficients to a 7th degree polynomial. It goes to scales of 8Mpc^{-1} and z&lt;=4.5.</li> <li>Cabayol23+_extended: Neural network emulating the optimal P1D of Gadget simulations fitting coefficients to a 5th degree polynomial. It goes to scales of 4Mpc^{-1} and z&lt;=4.5. Updated version compared to Cabayol+23 paper.</li> </ul> </li> <li>Nyx emulators:<ul> <li>Nyx_v0: Neural network emulating the optimal P1D of Nyx simulations fitting coefficients to a 6th degree polynomial. It goes to scales of 4Mpc^{-1} and z&lt;=4.5.</li> <li>Nyx_v0_extended: Neural network emulating the optimal P1D of Nyx simulations fitting coefficients to a 6th degree polynomial. It goes to scales of 8Mpc^{-1} and z&lt;=4.5.</li> <li>Nyx_alphap: Neural network emulating the optimal P1D of Nyx simulations fitting coefficients to a 6th degree polynomial. It goes to scales of 4Mpc^{-1} and z&lt;=4.5.</li> <li>Nyx_alphap_extended: Neural network emulating the optimal P1D of Nyx simulations fitting coefficients to a 6th degree polynomial. It goes to scales of 8Mpc^{-1} and z&lt;=4.5.</li> <li>Nyx_alphap_cov: Neural network under testing for the Nyx_alphap emulator.</li> </ul> </li> </ul> </li> <li> <p>Gaussian Process emulators:</p> <ul> <li>Gadget emulators:<ul> <li>\"Pedersen21\": Gaussian process emulating the optimal P1D of Gadget simulations. Pedersen+21 paper.</li> <li>\"Pedersen23\": Updated version of Pedersen21 emulator. Pedersen+23 paper.</li> <li>\"Pedersen21_ext\": Extended version of Pedersen21 emulator.</li> <li>\"Pedersen21_ext8\": Extended version of Pedersen21 emulator up to k=8 Mpc^-1.</li> <li>\"Pedersen23_ext\": Extended version of Pedersen23 emulator.</li> <li>\"Pedersen23_ext8\": Extended version of Pedersen23 emulator up to k=8 Mpc^-1.</li> </ul> </li> </ul> </li> </ul>"},{"location":"users/Emulators_trainingSets/#predefined-training-sets","title":"PREDEFINED TRAINING SETS","text":"<p>Similarly, LaCE provides a set of predefined training sets that have been used to train the emulators. These training sets correspond to a simulations suite, a postprocessing and the addition (or not) of mean flux rescalings. The training sets are:</p> <ul> <li>\"Pedersen21\": Training set used in Pedersen+21 paper. Gadget simulations without mean flux rescalings.</li> <li>\"Cabayol23\": Training set used in Cabayol+23 paper. Gadget simulations with mean flux rescalings and measuring the P1D along the three principal axes of the simulation box.</li> <li>\"Nyx_Oct2023\": Training set using Nyx version from October 2023.</li> <li>\"Nyx_Jul2024\": Training set using Nyx version from July 2024.</li> </ul>"},{"location":"users/Emulators_trainingSets/#connection-between-predefined-emulators-and-training-sets","title":"CONNECTION BETWEEN PREDEFINED EMULATORS AND TRAINING SETS","text":"<p>The following table shows the default training set for each predefined emulator.</p> Emulator Training Set Simulation Type Description Cabayol23 Cabayol23 Gadget NN Neural network emulator trained on Gadget simulations with mean flux rescaling Cabayol23+ Cabayol23 Gadget NN Updated version of Cabayol23 emulator Cabayol23_extended Cabayol23 Gadget NN Extended version of Cabayol23 emulator (k up to 8 Mpc^-1) Cabayol23+_extended Cabayol23 Gadget NN Extended version of Cabayol23+ emulator (k up to 8 Mpc^-1) Nyx_v0 Nyx_Oct2023 Nyx NN Neural network emulator trained on Nyx simulations Nyx_v0_extended Nyx_Oct2023 Nyx NN Extended version of Nyx_v0 emulator (k up to 8 Mpc^-1) Nyx_alphap Nyx_Oct2023 Nyx NN Neural network emulator trained on updated Nyx simulations Nyx_alphap_extended Nyx_Oct2023 Nyx NN Extended version of Nyx_alphap emulator (k up to 8 Mpc^-1) Nyx_alphap_cov Nyx_Jul2024 Nyx NN Testing version of Nyx_alphap emulator Pedersen21 Pedersen21 Gadget GP GP emulator trained on Gadget simulations without mean flux rescaling Pedersen23 Pedersen21 Gadget GP Updated version of Pedersen21 GP emulator Pedersen21_ext Pedersen21 Gadget GP Extended version of Pedersen21 GP emulator Pedersen21_ext8 Pedersen21 Gadget GP Extended version of Pedersen21 GP emulator (k up to 8 Mpc^-1) Pedersen23_ext Pedersen21 Gadget GP Extended version of Pedersen23 GP emulator Pedersen23_ext8 Pedersen21 Gadget GP Extended version of Pedersen23 GP emulator (k up to 8 Mpc^-1)"},{"location":"users/Simulations_list/","title":"AVAILABLE SIMULATIONS","text":"<p>This section contains the list of simulations available in the archives. </p>"},{"location":"users/Simulations_list/#gadget-simulations","title":"Gadget simulations","text":"<p>The Gadget simulations contain 30 training simulations, which are named as \"mpg_{x}\", where x is an integer number from 0 to 29. Additionaly, there are 7 test simulations:</p> <ul> <li>\"mpg_central\": The simulation parameters are at the center of the parameter space.</li> <li>\"mpg_neutrinos\": The simulation contains massive neutrinos.</li> <li>\"mpg_running\": The simulation has a non-zero running of the spectral index.</li> <li>\"mpg_growth\": The growth factor of the simulation is different from that of the training set.</li> <li>\"mpg_reio\": The reionization history is different from that of the training set.</li> <li>\"mpg_seed\": Identical to the central simulation with different initial conditions. Meant to test the impact of cosmic variance. </li> <li>\"mpg_curved\": The simulation has a different curvature power spectrum from that of the training set.</li> </ul> <p>For information about the simulation parameters can be found in Pedersen+21.</p>"},{"location":"users/Simulations_list/#nyx-simulations","title":"Nyx simulations","text":"<p>The Nyx simulation suite contains 18 training simulations, which are named as \"nyx_{x}\", where x is an integer number from 0 to 17. Additionally, there are 2 test simulations:</p> <ul> <li>\"nyx_central\": The simulation parameters are at the center of the parameter space.</li> <li>\"nyx_seed\": Identical to the central simulation with different initial conditions. Meant to test the impact of cosmic variance.</li> </ul> <p>For information about the simulation parameters can be found in TBD.</p>"},{"location":"users/archive/","title":"ARCHIVES","text":"<p>The LaCE emulators support two types of archives:</p> <ul> <li>Gadget archive: Contains the P1D of Gadget simulations described in Pedersen+21.  </li> <li>Nyx archive: Contains the P1D of Nyx simulations described in (In prep.)</li> </ul>"},{"location":"users/archive/#loading-a-gadget-archive","title":"Loading a Gadget Archive","text":"<p>The Gadget archive contains 30 training simulations and 7 test simulations. Each simulation contains 11 snapshotts covering redshifts from 2 to 4.5 in steps of 0.25.</p> <p>To laod a Gadget archive, you can use the <code>GadgetArchive</code> class:</p> <pre><code>from lace.archive.gadget_archive import GadgetArchive\n</code></pre> <p>The Gadget archive can be loaded with different post-processings: the one described in Pedersen+21, and the one from Cabayol+23.</p> <p>The P1D from the Gadget archive with the Pedersen+21 post-processing can be accessed as follows:</p> <pre><code>archive = GadgetArchive(postproc='Pedersen21')\n</code></pre> <p>This post-processing measures the P1D along one of the three box axes and does not contain mean-flux rescalings.</p> <p>On the other hand, the P1D from the Gadget archive with the Cabayol+23 post-processing can be accessed as follows:</p> <pre><code>archive = GadgetArchive(postproc='Cabayol23')\n</code></pre> <p>This post-processing measures the P1D along the three box principal axes and contains five mean-flux rescaling per snapshot.</p>"},{"location":"users/archive/#loading-a-nyx-archive","title":"Loading a Nyx Archive","text":"<p>To load the Nyx archive, you can use the <code>NyxArchive</code> class:</p> <pre><code>from lace.archive.nyx_archive import NyxArchive\n</code></pre> <p>Since the Nyx archive is not publicly available yet, you need to set the <code>NYX_PATH</code> environment variable to the path to the Nyx files on your local computer (or the cluster where you are running the code).</p> <p>There are two versions of the Nyx archive available: Oct2023 and Jul2024. The first one contains 17 training simulations and 4 test simulations, and the second one contains 17 training simulations and 3 test simulations (the simulations are better described here). Each simulation contains 14 snapshots covering redshifts from 2.2 to 4.8 in steps of 0.2 plus additional snapshotts at higher redshifts for some of the simulations. In both cases, it is not recommended to use simulation number 14. </p> <p>The P1D from the Nyx archive with the Oct2023 version can be accessed as follows:</p> <pre><code>archive = NyxArchive(nyx_version='Oct2023')\n</code></pre> <p>And the P1D from the Nyx archive with the Jul2024 version can be accessed as follows:</p> <pre><code>archive = NyxArchive(nyx_version='Jul2024')\n</code></pre>"},{"location":"users/archive/#accessing-the-training-set-and-the-test-set","title":"Accessing the Training Set and the Test Set","text":"<p>To access all data in the archive, you can use the <code>archive.data</code>. This will load all the snapshots and mean fluxes for all the simulations in the archive. </p> <p>If you want to access only the training set, you can use </p> <pre><code>archive.get_training_data(emu_params=emu_params)\n</code></pre> <p>and you will automatically load all snapshots and mean flux rescalings available in the training simulations.  </p> <p>For the test set, the equivalent function is:</p> <pre><code>archive.get_testing_data(sim_label='mpg_central')\n</code></pre> <p>where you can replace <code>sim_label</code> by any of the test simulation labels available in the archive (see here). This will only load the fiducial snapshots without mean flux rescalings. \u00a1</p>"},{"location":"users/compressedParameters/","title":"ESTIMATING COMPRESSED PARAMETERS WITH COSMOPOWER","text":"<p>To estimate the compressed parameters with cosmopower, one needs to follow the installation instructions of LaCE with cosmopower.</p> <ol> <li>Making predictions with cosmopower</li> <li>Estimating compressed parameters<ol> <li>For individual cosmologies </li> <li>For a cosmology chain</li> </ol> </li> <li>Training your own cosmopower emulator</li> </ol>"},{"location":"users/compressedParameters/#making-predictions-with-cosmopower","title":"Making predictions with cosmopower","text":"<p>To see examples of how to make predictions with cosmopower, one can follow the tutorial notebook in the notebooks folder (Tutorial_compressed_parameters.ipynb).</p> <p>Cosmopower emulates the linear matter power spectrum. To use the emulator, one needs to provide a set of cosmological parameters that depend on the cosmological parameters used to train the emulator. LaCE contains several trained cosmopower emulators, and you can also train your own emulator as described in Training your own cosmopower emulator.</p> <p>To load an emulator, you need to do: </p> <pre><code>cp_nn = cp.cosmopower_NN(restore=True, \n                         restore_filename=emu_path)\n</code></pre> <p>There are three trained cosmopower emulators that you can find in the data folder: - <code>Pk_cp_NN.pkl</code>: $\\Lambda$CDM emulator. - <code>Pk_cp_NN_sumnu.pkl</code>: $\\Lambda$ CDM + $\\sum m_\\nu$ emulator. - <code>Pk_cp_NN_nrun.pkl</code>: $\\Lambda$ CDM + $\\sum m_\\nu$ + $n_{\\rm run}$ emulator.</p> <p>When providing the path to the emulator, you need to give the path to the <code>Pk_cp_NN.pkl</code> or <code>Pk_cp_NN_sumnu.pkl</code> file without the <code>.pkl</code> extension.</p> <p>To know the parameters that the emulator uses, you can do:</p> <pre><code>print(cp_nn.parameters())\n</code></pre> <p>And to access the k values, you can do:</p> <pre><code>print(cp_nn.modes)\n</code></pre>"},{"location":"users/compressedParameters/#making-predictions","title":"Making predictions","text":"<p>To make predictions with cosmopower, you need to provide a dictionary with the parameters that the emulator uses.</p> <pre><code># Define the cosmology dictionary\ncosmo = {'H0': [cosmo_params[\"H0\"]],\n         'h': [cosmo_params[\"H0\"]/100],\n         'mnu': [cosmo_params[\"mnu\"]],\n         'Omega_m': [(cosmo_params[\"omch2\"] + cosmo_params[\"ombh2\"]) / (cosmo_params[\"H0\"]/100)**2],\n         'Omega_Lambda': [1- (cosmo_params[\"omch2\"] + cosmo_params[\"ombh2\"]) / (cosmo_params[\"H0\"]/100)**2],\n         'omch2': [cosmo_params[\"omch2\"]],\n         'ombh2': [cosmo_params[\"ombh2\"]],\n         'As': [cosmo_params[\"As\"]],\n         'ns': [cosmo_params[\"ns\"]],\n         'nrun': [cosmo_params[\"nrun\"]]}\n</code></pre> <p>Some of these parameters are not used by the emulator, but they are needed to convert to km/s. When calling the emulator, it will inform you if some of the parameters are not used.</p> <p>To call the emulator:</p> <pre><code>Pk_Mpc = cp_nn.ten_to_predictions_np(cosmo)\n</code></pre> <p>Then to convert to km/s, you can do:</p> <pre><code>k_kms, Pk_kms = linPCosmologyCosmopower.convert_to_kms(cosmo, \n                                                       k_Mpc, \n                                                       Pk_Mpc, \n                                                       z_star = z_star)\n</code></pre>"},{"location":"users/compressedParameters/#estimating-compressed-parameters","title":"Estimating compressed parameters","text":""},{"location":"users/compressedParameters/#for-indivudual-cosmologies","title":"For indivudual cosmologies","text":"<p>Once you have the predictions, you can estimate the compressed parameters fitting a polynomial to the power spectrum:</p> <pre><code>linP_kms = linPCosmologyCosmopower.fit_polynomial(\n    xmin = kmin_kms / kp_kms, \n    xmax= kmax_kms / kp_kms, \n    x = k_kms / kp_kms, \n    y = Pk_kms, \n    deg=2\n)\n</code></pre> <p>and then estimate the star parameters:</p> <pre><code>starparams_CP = linPCosmologyCosmopower.get_star_params(linP_kms = linP_kms, \n                                                          kp_kms = kp_kms)\n</code></pre> <p>where <code>linP_kms</code> are the linear matter power spectrum predictions in km/s and <code>kp_kms</code> is the pivot point in s/km. </p>"},{"location":"users/compressedParameters/#for-a-cosmology-chain","title":"For a cosmology chain","text":"<p>To estimate the parameters for a cosmology chain, you first need to call the class:</p> <pre><code>fitter_compressed_params = linPCosmologyCosmopower(cosmopower_model = cosmopower_model)\n</code></pre> <p>Then check the expected parameters of the cosmopower model:</p> <pre><code>print(fitter_compressed_params.cp_emulator.cp_emulator.parameters())\n</code></pre> <p>And create a dictionary with the naming convertion between the parameters in your chain and the parameters used by the cosmopower model. We need additional parameters to convert to km/s. The dictionary keys are the parameters used by the cosmopower model and the values are the parameters in your chain.</p> <pre><code>param_mapping = {\n    'h': 'h',\n    'm_ncdm': 'm_ncdm',\n    'omch2': 'omega_cdm',\n    'Omega_m': 'Omega_m',\n    'Omega_Lambda': 'Omega_Lambda',\n    'ln_A_s_1e10': 'ln_A_s_1e10',\n    'ns': 'n_s',\n    'nrun': 'nrun'\n}\n</code></pre> <p>To estimate the parameters, you can do:</p> <pre><code>linP_cosmology_results = fitter_compressed_params.fit_linP_cosmology(chains_df = df, \n                                                                     param_mapping = param_mapping)\n</code></pre> <p>By default in loads the $\\Lambda$CDM + $\\sum m_\\nu$ + $n_{\\rm run}$ emulator. If you want to use another emulator, specify the name in the class argument <code>cosmopower_model</code> and save the model in the <code>data/cosmopower_models</code> folder. The model needs to be a <code>.pkl</code> file and the it must be called without the <code>.pkl</code> extension.</p>"},{"location":"users/compressedParameters/#training-your-own-cosmopower-emulator","title":"Training your own cosmopower emulator","text":"<p>To train your own cosmopower emulator, you can follow the tutorial notebook.</p> <p>First, one needs to create a LH sampler with the parameters of interest. For example, to create a LH sampler with the $\\Lambda$CDM + $\\sum m_\\nu$ emulator, you can do:</p> <pre><code>#Define the parameters used by the emulator and the ranges\nparams = [\"ombh2\", \"omch2\", \"H0\", \"ns\", \"As\", \"mnu\", \"nrun\"]\nranges = [\n    [0.015, 0.03],\n    [0.05, 0.16], \n    [60, 80],\n    [0.8, 1.2],\n    [5e-10, 4e-9],\n    [0, 2],\n    [0, 0.05]\n]\ndict_params_ranges = dict(zip(params, ranges))\n</code></pre> <p>And create the LH sample as:</p> <pre><code>create_LH_sample(dict_params_ranges = dict_params_ranges,\n                     nsamples = 10_000,\n                     filename = \"LHS_params_sumnu.npz\")\n</code></pre> <p>After that, you need to create the power spectrum to train the emulator. </p> <pre><code>generate_training_spectra(input_LH_filename = 'LHS_params_sumnu.npz',\n                          output_filename = \"linear_sumnu.dat\")\n</code></pre> <p>Make sure that the input file is the one you created in the previous step. This calls CAMB to generate the power spectrum. It takes time to run. So far, we have not needed a long training sample to reach good accuracy.</p> <p>Once the power spectrum is generated, you can train the emulator with:</p> <pre><code>cosmopower_prepare_training(params = params,\n                            Pk_filename = \"linear_test.dat\")\n</code></pre> <p>followed by:</p> <pre><code>cosmopower_train_model(\n                        model_save_filename = \"Pk_cp_NN_test\",\n                        model_params = params\n)\n</code></pre>"},{"location":"users/emulatorPredictions/","title":"MAKING PREDICTIONS WITH LACE","text":""},{"location":"users/emulatorPredictions/#loading-a-predefined-emulator","title":"Loading a predefined emulator","text":"<p>The easiest way to load an emulator is to use the <code>set_emulator</code> class. This can be done with an archive or a training set. This will load a trained emulator with the specifications of the emulator label.</p> <pre><code>archive = gadget_archive.GadgetArchive(postproc=\"Pedersen21\")\nemulator = set_emulator(\n        emulator_label=\"Pedersen23_ext\",\n        archive=archive,\n    )\n</code></pre> <pre><code>archive = gadget_archive.GadgetArchive(postproc=\"Pedersen21\")\nemulator = set_emulator(\n        emulator_label=\"Pedersen23_ext\",\n        training_set=\"Pedersen21\",\n    )\n</code></pre> <p>The supported emulators can be found in the Emulators_trainingSets.md file.</p>"},{"location":"users/emulatorPredictions/#loading-a-custom-emulator","title":"Loading a custom emulator","text":"<p>Another option is to load an emulator model that does not correspond to a predifined emulator label. This can be done by, for example:</p> <pre><code>emulator = NNEmulator(training_set='Cabayol23', \n            emulator_label='Cabayol23+',\n            model_path='path/to/model.pt',\n            drop_sim=None,\n            train=False)\n</code></pre> <p>where <code>model_path</code> is the path to the <code>.pt</code> file containing the trained model and <code>train=False</code> indicates that the model is not being trained. In the model you are loading has been trained by dropping simulations, you should specify which simulations to drop using the <code>drop_sim</code> argument.</p>"},{"location":"users/emulatorPredictions/#making-predictions","title":"Making predictions","text":"<p>To emulate the P1D of a simulation, you can use the <code>emulate_p1d_Mpc</code> method. This method requires a dictionary containing the simulation parameters.</p> <pre><code>p1d = emulator.emulate_p1d_Mpc(sim_params, k_Mpc)\n</code></pre>"},{"location":"users/emulatorPredictions/#predicitng-from-a-config-file","title":"Predicitng from a config file","text":"<p>One can also make predictions and plot them by using the <code>predict.py</code> script in the <code>scripts</code> folder. This script allows to make predictions on a test set, plot the P1D errors and save the predictions. An example of how to use this script is:</p> <p><code>bash python python bin/predict.py --config config_files/config_predict.yaml</code> Similarly to the training script, the config file accepts the following fields:</p> <p>There are two ways of loading an emulator: 1. By providing an emulator label (see list of supported emulators here) - <code>emulator_type</code>: Choose between \"NN\" (neural network) or \"GP\" (Gaussian process) emulator - <code>emulator_label</code>: Label of the predefined model to use (see list of supported emulators here)  - <code>drop_sim</code>: Simulation to exclude from training (optional) - <code>archive</code>: Configuration for loading simulation archive   - <code>file</code>: \"Nyx\" or \"Gadget\"   - <code>version</code>: Version of the simulation archive   - <code>sim_test</code>: Label of the test simulation to use for predictions. See list of available simulations here. - <code>average_over_z</code>: Whether to average P1D errors over redshift (true/false) - <code>save_plot_path</code>: Path where to save the validation plot. If None, the plot is not saved. - <code>save_predictions_path</code>: Path where to save the predictions. If None, the predictions are not saved.</p> <ol> <li>By providing a path to the directory containing the trained model (this directory should contain the <code>.pt</code> file.</li> <li><code>emulator_type</code>: Choose between \"NN\" (neural network) or \"GP\" (Gaussian process) emulator</li> <li><code>drop_sim</code>: Simulation to exclude from training (optional)</li> <li><code>archive</code>: Configuration for loading simulation archive</li> <li><code>file</code>: \"Nyx\" or \"Gadget\"</li> <li><code>version</code>: Version of the simulation archive</li> <li><code>sim_test</code>: Label of the test simulation to use for predictions. See list of available simulations here.</li> <li><code>emulator_params</code>: List of parameters used by the emulator. The default is <code>[\"Delta2_p\", \"n_p\", \"alpha_p\", \"sigT_Mpc\", \"gamma\", \"kF_Mpc\"]</code>.</li> <li><code>average_over_z</code>: Whether to average P1D errors over redshift (true/false)</li> <li><code>save_plot_path</code>: Path where to save the validation plot. If None, the plot is not saved.</li> <li><code>save_predictions_path</code>: Path where to save the predictions. If None, the predictions are not saved.</li> <li><code>hyperparameters</code>: Dictionary containing the hyperparameters of the emulator. This will be used ONLY if <code>emulator_label</code> is not provided.</li> <li><code>model_path</code>: Path to the directory containing the trained model (this directory should contain the <code>.pt</code> file).</li> </ol>"},{"location":"users/emulatorTraining/","title":"EMULATOR TRAINING","text":"<p>The training of the emulators is done with the code <code>train.py</code>. which is in the <code>scripts</code> folder. This code is used to train the emulators with the data available in the repository. </p> <p>In order to train the emulator, one needs to specify the training configuration file. This file is a .yaml file that contains the parameters for the training. An example of this file is <code>config.yaml</code> in the <code>config_files</code> folder. </p> <p>To run the training, one needs to run the following command:</p> <pre><code>python scripts/train.py --config=path/to/config.yaml\n</code></pre>"},{"location":"users/emulatorTraining/#configuration-file-instructions","title":"Configuration file instructions","text":"<p>The configuratoin file contains the following parameters:</p> <ol> <li> <p><code>emulator_type</code>: Specifies the emulator type, either \"NN\" or \"GP\".</p> </li> <li> <p><code>emulator_label</code>: Specifies the predefined model to train. For example, \"Cabayol23+\". This parameter is optional, if it is not provided, the code will train a new model based on the provided hyperparameters. The options from the emulator_label are defined in the Emulators_trainingSets.md file.</p> </li> <li> <p>Data source (choose one): The data source can be either an archive or a predefined training set. For archive, the following options are available:    a. <code>archive</code>:</p> <ul> <li><code>file</code>: Specifies the simulation type, either \"Nyx\" or \"Gadget\".</li> <li><code>version</code>: Specifies the Nyx version or Gadget postprocessing version. Options are:<ul> <li>\"Pedersen21\": Gadget postprocessing in Pedersen+21 paper.</li> <li>\"Cabayol23\": Gadget postprocessing in Cabayol+23 paper.</li> <li>\"Nyx23_Oct2023\": Nyx version from October 2023.</li> <li>\"Nyx23_Jul2024\": Nyx version from July 2024.    b. <code>training_set</code>: Specifies a predefined training set. The options are defined in the Emulators_trainingSets.md file.    There is the option of dropping simulations from the training set or archive. This is done providing a simulation to the <code>drop_sim</code> parameter. The simulations are listed in Simulations_list.md.</li> </ul> </li> </ul> </li> <li> <p><code>emulator_params</code>: List of parameters the emulator will use for predictions. By default, it uses ['Delta2_p', 'n_p','mF', 'sigT_Mpc', 'gamma', 'kF_Mpc']. Nyx_alphap emulators use ['Delta2_p', 'n_p','mF', 'sigT_Mpc', 'gamma', 'kF_Mpc', 'alpha_p'].</p> </li> <li> <p><code>drop_sim</code>: Simulation to exclude from training (optional).</p> </li> <li> <p><code>training_set</code>: Predefined training set to use for training. The options are defined in the Emulators_trainingSets.md](./Emulators_trainingSets.md) file.</p> </li> <li> <p><code>save_path</code>: Location to save the trained model, relative to the project root directory.</p> </li> <li> <p><code>hyperparameters</code>: Neural network and training configuration. These parameters need to be specified when the emulator_label is not provided. If the emulator_label is provided, the hyperparameters will be taken from the predefined emulator. The parameters are:</p> </li> </ol> <p>\u2022 <code>kmax_Mpc</code>: Maximum k value in Mpc^-1 to consider.</p> <p>\u2022 <code>ndeg</code>: Degree of the polynomial fit.</p> <p>\u2022 <code>nepochs</code>: Number of training epochs.</p> <p>\u2022 <code>step_size</code>: Number of epochs between learning rate adjustments.</p> <p>\u2022 <code>drop_sim</code>: Simulation to exclude from training (optional).</p> <p>\u2022 <code>drop_z</code>: Redshift to exclude from training (optional).</p> <p>\u2022 <code>nhidden</code>: Number of hidden layers.</p> <p>\u2022 <code>max_neurons</code>: Maximum number of neurons per layer.</p> <p>\u2022 <code>seed</code>: Random seed for reproducibility.</p> <p>\u2022 <code>lr0</code>: Initial learning rate.</p> <p>\u2022 <code>batch_size</code>: Number of samples per batch.</p> <p>\u2022 <code>weight_decay</code>: L2 regularization factor.</p> <p>\u2022 <code>z_max</code>: Maximum redshift to consider.</p> <p>To train the GP emulator, the <code>emulator_type</code> parameter is set to \"GP\" and the <code>emulator_label</code> to one of the predefined emulators in the Emulators_trainingSets.md file. The GP emulator on the Nyx archive is not supported. The hyperparameters used by the GP emulator are:</p> <ul> <li><code>kmax_Mpc</code>: Maximum k value in Mpc^-1 to consider.</li> <li><code>ndeg</code>: Degree of the polynomial fit.</li> <li><code>drop_sim</code>: Simulation to exclude from training (optional).</li> <li><code>z_max</code>: Maximum redshift to consider.</li> </ul>"}]}